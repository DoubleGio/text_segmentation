{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, tarfile, re\n",
    "from typing import List, Optional\n",
    "import folia.main as folia\n",
    "from folia import fql\n",
    "from utils import NLNEWS_LOC, subdivide_dir\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "rng = np.random.default_rng()\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "base = '../NLNews/WR-P-P-G_newspapers/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the tarfiles\n",
    "* Download the [SoNaR corpus](https://taalmaterialen.ivdnt.org/download/tstc-sonar-corpus/) (60+GB).\n",
    "* Extract newspaper files to separate archive.\n",
    "    * Delete full Sonar corpus (as it's kinda big).\n",
    "* Extract text from new archive to new folder.\n",
    "* Finally create multi-segmented docs.\n",
    "\n",
    "https://stackoverflow.com/questions/17616340/add-files-from-one-tar-into-another-tar-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SoNaR corpus --> Newspapers corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the newspaper files.\n",
    "# Inside the tarfile, newspapers folia-files are stored as './SoNaRCorpus_NC_1.2/SONAR500/FoLiA/WR-P-P-G_newspapers/###/WR-P-P-G-##########.folia.xml'\n",
    "# Takes ~30 mins.\n",
    "archive = tarfile.open(\"../Datasets/NLNews/20150602_SoNaRCorpus_NC_1.2.1.tgz\")\n",
    "select = [tarinfo for tarinfo in archive if tarinfo.name.startswith('./SoNaRCorpus_NC_1.2/SONAR500/FoLiA/WR-P-P-G_newspapers/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '000/sonar-foliaviewer.xsl', '000/WR-P-P-G-0000000001.folia.xml', '000/WR-P-P-G-0000000002.folia.xml', '000/WR-P-P-G-0000000003.folia.xml', '000/WR-P-P-G-0000000004.folia.xml', '000/WR-P-P-G-0000000005.folia.xml', '000/WR-P-P-G-0000000006.folia.xml', '000/WR-P-P-G-0000000007.folia.xml', '000/WR-P-P-G-0000000008.folia.xml']\n"
     ]
    }
   ],
   "source": [
    "# Create the new archive and add the selected files to it.\n",
    "# Takes +1h.\n",
    "# NOTE: possible to parellize? (https://stackoverflow.com/questions/13446445/python-multiprocessing-safely-writing-to-a-file & https://stackoverflow.com/questions/43313666/python-parallel-processing-to-unzip-files)\n",
    "with tarfile.open(\"../Datasets/NLNews/WR-P-P-G_newspapers.tgz\", \"x:gz\") as new_archive:\n",
    "    for member in select:\n",
    "        member.name = member.name.replace('./SoNaRCorpus_NC_1.2/SONAR500/FoLiA/WR-P-P-G_newspapers/', '')\n",
    "        new_archive.addfile(member, archive.extractfile(member))\n",
    "    print(new_archive.getnames()[:10])\n",
    "archive.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newspaper corpus --> txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_txt(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the text from a folia.xml document, filtering out paragraphs containing 10 or less words (this includes things such as image captions) and footnotes (e.g. '( ANP )').\n",
    "    https://foliapy.readthedocs.io/en/latest/folia.html#\n",
    "    \"\"\"\n",
    "    doc = folia.Document(file=filepath)\n",
    "    paragraphs = [p.text() for p in doc.select(folia.Paragraph) if p.count(folia.Word) > 10]\n",
    "    if len(paragraphs) == 0:\n",
    "        return ''\n",
    "    # Sometimes articles start with: \"LOCATION -\"; the following removes it.\n",
    "    # Look for '-' in first sentence of the first paragraph.\n",
    "    query = fql.Query(f'SELECT w WHERE text = \"-\" IN ID {doc.paragraphs(0)[0].id}')\n",
    "    found = query(doc)\n",
    "    if found:\n",
    "        pp = doc.paragraphs(0).copy()\n",
    "        word = found[0]\n",
    "        # Walk backwards, only if the word directly before the '-' is a capitalized location.\n",
    "        while True:\n",
    "            prev = word.previous(folia.Word)\n",
    "            if prev is None:\n",
    "                pp[0].remove(word)\n",
    "                break\n",
    "            if prev.annotation(folia.PosAnnotation, set=\"http://ilk.uvt.nl/folia/sets/frog-mbpos-cgn\").cls == 'SPEC(deeleigen)' and prev.text().isupper():\n",
    "                pp[0].remove(word)\n",
    "                word = prev\n",
    "                prev = word.previous()\n",
    "            else:\n",
    "                break\n",
    "        try:\n",
    "            paragraphs[0] = pp.text()\n",
    "        except folia.NoSuchText:\n",
    "            paragraphs = paragraphs[1:]\n",
    "    res = ' '.join(paragraphs)\n",
    "    res = re.sub(r'\\( \\w+ \\).?$', '\\n', res, flags=re.DOTALL)\n",
    "    return f'==={doc.id}===\\n{res}\\n'\n",
    "\n",
    "class Extractor(mp.Process):\n",
    "    def __init__(self, tar_loc: str, temp_loc: str, txt_loc: str, in_q: mp.Queue, out_q: mp.Queue, name: str):\n",
    "        super().__init__()\n",
    "        self.tar_loc = tar_loc\n",
    "        self.temp_loc = temp_loc\n",
    "        self.txt_loc = txt_loc\n",
    "        self.in_q = in_q\n",
    "        self.out_q = out_q\n",
    "        self.name = name\n",
    "\n",
    "    def run(self):\n",
    "        print(f'Starting {self.name}')\n",
    "        with tarfile.open(self.tar_loc, 'r:gz') as tar:\n",
    "            while True:\n",
    "                tarinfo = self.in_q.get()\n",
    "                if tarinfo is None:\n",
    "                    break\n",
    "                if tarinfo.name.endswith('.folia.xml'):\n",
    "                    tarinfo.name = tarinfo.name.split('/')[1]\n",
    "                    new_loc = os.path.join(self.txt_loc, tarinfo.name.split('.')[0])\n",
    "                    if not os.path.exists(new_loc): # Skip already extracted files.\n",
    "                        tar.extract(tarinfo, path=self.temp_loc)\n",
    "                        self.out_q.put(tarinfo.name)\n",
    "        return\n",
    "\n",
    "def writer(q: mp.Queue, temp_loc: str, txt_loc: str):\n",
    "    print(f'Starting {mp.current_process().name}')\n",
    "    while True:\n",
    "        name = q.get()\n",
    "        if name is None:\n",
    "            break\n",
    "        temp_file = os.path.join(temp_loc, name)\n",
    "        text = create_txt(temp_file)\n",
    "        if text:\n",
    "            new_loc = os.path.join(txt_loc, name.split('.')[0])\n",
    "            with open(new_loc, 'w') as f:\n",
    "                f.write(text)\n",
    "        os.remove(temp_file)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Starting extraction===\n",
      "Found 708979 files.\n",
      "Starting extractor_0\n",
      "Starting extractor_1\n",
      "Starting extractor_2\n",
      "Starting extractor_3\n",
      "Starting extractor_4\n",
      "Starting extractor_5\n",
      "\n",
      "Starting extractor_6Starting extractor_7\n",
      "Starting writer_0\n",
      "Starting writer_1\n",
      "Starting writer_2\n",
      "Starting writer_3\n",
      "Starting writer_4\n",
      "Starting writer_5\n",
      "Starting writer_6\n",
      "Starting writer_7\n",
      "Starting writer_8\n",
      "Starting writer_9\n",
      "Starting writer_10\n",
      "Starting writer_11\n",
      "Starting writer_12\n",
      "Starting writer_13\n",
      "Starting writer_14\n",
      "Starting writer_15\n"
     ]
    }
   ],
   "source": [
    "# Extract the text from each of the folia-files and puts them into a seperate folder.\n",
    "# Takes a while.\n",
    "temp_loc = '../Datasets/NLNews/temp'\n",
    "txt_loc = '../Datasets/NLNews/WR-P-P-G_newspapers_txt'\n",
    "tar_loc = '../Datasets/NLNews/WR-P-P-G_newspapers.tgz'\n",
    "if not os.path.exists(temp_loc):\n",
    "    os.mkdir(temp_loc)\n",
    "if not os.path.exists(txt_loc):\n",
    "    os.mkdir(txt_loc)\n",
    "\n",
    "num_procs = int(mp.cpu_count() / 4)\n",
    "extract_q = mp.Queue()\n",
    "file_q = mp.Queue(maxsize=num_procs*4)\n",
    "\n",
    "print('===Starting extraction===')\n",
    "with tarfile.open(tar_loc, 'r:gz') as tar:\n",
    "    # Check if the foliaviewer.xsl file exists in side temp_loc; it is required for reading the folia-files.\n",
    "    if not os.path.exists(os.path.join(temp_loc, 'sonar-foliaviewer.xsl')):\n",
    "        foliaviewer = tar.getmember('000/sonar-foliaviewer.xsl')\n",
    "        foliaviewer.name = 'sonar-foliaviewer.xsl'\n",
    "        tar.extract(foliaviewer, temp_loc)\n",
    "    try:\n",
    "        members\n",
    "    except NameError:\n",
    "        print('    Extracting members...')\n",
    "        members = tar.getmembers()\n",
    "size = len(members)\n",
    "print(f'Found {size} files.')\n",
    "for i in range(num_procs): members.append(None)\n",
    "for member in members: extract_q.put(member)\n",
    "time.sleep(1) # Fixes brokenpipe error\n",
    "\n",
    "extractors = [Extractor(tar_loc, temp_loc, txt_loc, extract_q, file_q, f'extractor_{i}') for i in range(num_procs)]\n",
    "writers = [mp.Process(name=f'writer_{i}', target=writer, args=(file_q, temp_loc, txt_loc)) for i in range(num_procs*2)]\n",
    "for e in extractors: e.start()\n",
    "for w in writers: w.start()\n",
    "\n",
    "for e in extractors: e.join()\n",
    "for w in writers: file_q.put(None)\n",
    "for w in writers: w.join()\n",
    "subdivide_dir(txt_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in extractors: e.kill()\n",
    "for w in writers: w.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-parallel code:\n",
    "# Takes absolutely forever...\n",
    "temp_loc = '../Datasets/NLNews/temp'\n",
    "txt_loc = '../Datasets/NLNews/WR-P-P-G_newspapers_txt'\n",
    "tar_loc = '../Datasets/NLNews/WR-P-P-G_newspapers.tgz'\n",
    "if not os.path.exists(temp_loc):\n",
    "    os.mkdir(temp_loc)\n",
    "if not os.path.exists(txt_loc):\n",
    "    os.mkdir(txt_loc)\n",
    "with tarfile.open('../NLNews/WR-P-P-G_newspapers.tgz', 'r:gz') as new_archive:\n",
    "    # Check if the foliaviewer.xsl file exists in side temp_loc; it is required for reading the folia-files.\n",
    "    if not os.path.exists(os.path.join(temp_loc, 'sonar-foliaviewer.xsl')):\n",
    "        foliaviewer = new_archive.getmember('000/sonar-foliaviewer.xsl')\n",
    "        foliaviewer.name = 'sonar-foliaviewer.xsl'\n",
    "        new_archive.extract(foliaviewer, temp_loc)\n",
    "    \n",
    "    for tarinfo in new_archive:\n",
    "        if tarinfo.name.endswith('.folia.xml'):\n",
    "            tarinfo.name = tarinfo.name.replace(cur_folder + '/', '')\n",
    "            new_loc = os.path.join(txt_loc, tarinfo.name.split('.')[0])\n",
    "            if not os.path.exists(new_loc): # Skip already extracted files.\n",
    "                new_archive.extract(tarinfo, path=temp_loc)\n",
    "                text = create_txt(os.path.join(temp_loc, tarinfo.name))\n",
    "                if text:\n",
    "                    with open(new_loc, 'w') as f:\n",
    "                        f.write(text)\n",
    "                os.remove(os.path.join(temp_loc, tarinfo.name))\n",
    "        elif tarinfo.isdir():\n",
    "            cur_folder = tarinfo.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WR-P-P-G-0000434148.folia.xml'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check last temp file.\n",
    "# with open(f\"{temp_loc}/{os.listdir(temp_loc)[-1]}\", 'r') as f:\n",
    "#     print(f.read())\n",
    "os.listdir(temp_loc)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create multi-segment documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List containing the location of all files\n",
    "txt_loc = '../Datasets/NLNews/WR-P-P-G_newspapers_txt'\n",
    "txt_locs = [os.path.join(root,file) for root, _, files in os.walk(txt_loc) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SECTIONS = 2\n",
    "MAX_SECTIONS = 5\n",
    "\n",
    "def create_docs(locs: List[str], save=True) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Creates a folder containing .txt files from concatenated articles.\n",
    "    :param locs: List/np.ndarray containing locations to the .txt files to be processed.\n",
    "    :param save: Whether to save to a folder or to return the results as a list of Strings.\n",
    "    \"\"\"\n",
    "    if isinstance(locs, List):\n",
    "        locs = np.array(locs)\n",
    "\n",
    "    i = 0\n",
    "    pad = int(np.log10(len(locs))) + 1\n",
    "\n",
    "    if save:\n",
    "        if not os.path.exists(NLNEWS_LOC):\n",
    "            os.mkdir(NLNEWS_LOC)\n",
    "    else:\n",
    "        docs = []\n",
    "\n",
    "    with tqdm(total=locs.size, desc='Articles processed') as pbar:\n",
    "        while locs.size > 0:\n",
    "            if locs.size > MAX_SECTIONS + 1:\n",
    "                n = rng.integers(MIN_SECTIONS, MAX_SECTIONS + 1)\n",
    "            elif locs.size > MAX_SECTIONS:\n",
    "                n = rng.integers(MIN_SECTIONS, MAX_SECTIONS)\n",
    "            else:\n",
    "                n = locs.size\n",
    "            slice = rng.choice(locs.size, size=n, replace=False)\n",
    "\n",
    "            doc = ''\n",
    "            for l in locs[slice]:\n",
    "                with open(l, 'r') as f:\n",
    "                    doc += f.read()\n",
    "            locs = np.delete(locs, slice)\n",
    "\n",
    "            if save:\n",
    "                with open(f'{NLNEWS_LOC}/{i:0{pad}}', 'w') as tfile:\n",
    "                    tfile.write(doc)\n",
    "            else:\n",
    "                docs.append(doc)\n",
    "            i += 1\n",
    "            pbar.update(n)\n",
    "\n",
    "    print(f'Created {i} Documents')\n",
    "    if save is False: \n",
    "        return docs\n",
    "\n",
    "# NOTE: could be parellized.\n",
    "# def subdivide_list(l: List, n: int) -> List:\n",
    "#     \"\"\"\n",
    "#     Subdivides a list into n sublists.\n",
    "#     :param l: List to subdivide.\n",
    "#     :param n: Number of sublists.\n",
    "#     :return: List of sublists.\n",
    "#     \"\"\"\n",
    "#     sub_size, leftover = divmod(len(l), n)\n",
    "#     rng.shuffle(l)\n",
    "#     res = [l[sub_size * i:sub_size * (i + 1)] for i in range(n)]\n",
    "#     if leftover:\n",
    "#         res[-1].extend(l[-leftover:])\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Articles processed: 100%|██████████| 696609/696609 [2:26:02<00:00, 79.50it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 199153 Documents\n"
     ]
    }
   ],
   "source": [
    "# Takes 2+ hours to finish.\n",
    "create_docs(txt_locs)\n",
    "subdivide_dir(NLNEWS_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b87c1d3ab45aa43570b770b85056e3932dbddf43e01c1d13aa1a9a9935e3e26"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('gio': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
