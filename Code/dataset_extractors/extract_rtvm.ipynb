{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New dataset\n",
    "1. List of ids from jsons\n",
    "2. Retrieve metadata from manifest file\n",
    "3. Create dataframe wih timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, sys, json\n",
    "import numpy as np\n",
    "from typing import Optional, List, Tuple\n",
    "from tqdm import tqdm\n",
    "rng = np.random.default_rng()\n",
    "sys.path.append('../')\n",
    "from utils import subdivide_dir, NLAUVI_LOC_C, NLAUVI_LOC_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '../../../../data/samenvattingen/'\n",
    "base_csv = '../../../../data/samenvattingen/ArchiefExport_'\n",
    "years = ['2019', '2020']\n",
    "# path_csv = '../../../data/samenvattingen/ArchiefExport_2019.csv'\n",
    "# path_2019 = \"../../../data/samenvattingen/2019\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process manifest file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed files: 111232\n"
     ]
    }
   ],
   "source": [
    "# Get IDs & file location from processed files\n",
    "dfs = []\n",
    "for year in years:\n",
    "    id_dict = {}\n",
    "    for root, _, files in os.walk(base + year):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                id_dict[file.split(\".\")[0]] = os.path.join(root, file)\n",
    "\n",
    "    # Retrieve only processed IDs\n",
    "    df = pd.read_csv(base_csv + year + \".csv\")\n",
    "    df = df[df[\"ID\"].isin(id_dict.keys())].copy()\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['Path'] = df['ID'].apply(lambda id: id_dict[id])\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "# Drop irrelevant columns\n",
    "df = df.drop(columns=[\"AlertTime\", \"SpeechUrl\", \"MediaUrl\", \"Summary\"], errors='ignore')\n",
    "del dfs\n",
    "print(f'Number of processed files: {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "def get_ms(timestamp):\n",
    "   \"\"\"\n",
    "   Transform timestamp from minutes into miliseconds\n",
    "   \"\"\"\n",
    "   ms = 0\n",
    "   timestamp = timestamp.split(\".\")[0]\n",
    "   timestamp = timestamp.split(\":\")\n",
    "   ms += (int(timestamp[1]) * 60 + int(timestamp[2])) * 1000\n",
    "   return ms\n",
    "  \n",
    "def adj_start_ms(x):\n",
    "   \"\"\"\"\n",
    "   Recording start ~5 minutes (30000ms) before segment starts, but recorded times can be any time within a broadcast.\n",
    "   This shifts start times to 5 min or less. Extra 10000ms is added for robustness.\n",
    "   \"\"\"\n",
    "   if x > 300000:\n",
    "       return 310000\n",
    "   else:\n",
    "       return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ProgrammeDateTime</th>\n",
       "      <th>Path</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>adj_start_ms</th>\n",
       "      <th>adj_end_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E94B5735-A16D-41A9-A05A-09EE40E240A3</td>\n",
       "      <td>2019-01-01 05:00:00.000</td>\n",
       "      <td>../../../../data/samenvattingen/2019/01/01/E94...</td>\n",
       "      <td>331000</td>\n",
       "      <td>310000</td>\n",
       "      <td>641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168D1E30-8F21-4C9D-A5DD-149D77E233B1</td>\n",
       "      <td>2019-01-01 06:00:00.000</td>\n",
       "      <td>../../../../data/samenvattingen/2019/01/01/168...</td>\n",
       "      <td>17000</td>\n",
       "      <td>137000</td>\n",
       "      <td>154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FCDD8B88-9BC5-4F84-BA71-610BFA889314</td>\n",
       "      <td>2019-01-01 06:00:00.000</td>\n",
       "      <td>../../../../data/samenvattingen/2019/01/01/FCD...</td>\n",
       "      <td>17000</td>\n",
       "      <td>163000</td>\n",
       "      <td>180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9F461187-A082-42DF-B09B-7566B290657F</td>\n",
       "      <td>2019-01-01 06:00:00.000</td>\n",
       "      <td>../../../../data/samenvattingen/2019/01/01/9F4...</td>\n",
       "      <td>486000</td>\n",
       "      <td>310000</td>\n",
       "      <td>796000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3376B065-A0BD-4E3D-AC05-14B278077F90</td>\n",
       "      <td>2019-01-01 07:00:00.000</td>\n",
       "      <td>../../../../data/samenvattingen/2019/01/01/337...</td>\n",
       "      <td>333000</td>\n",
       "      <td>310000</td>\n",
       "      <td>643000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID        ProgrammeDateTime  \\\n",
       "0  E94B5735-A16D-41A9-A05A-09EE40E240A3  2019-01-01 05:00:00.000   \n",
       "1  168D1E30-8F21-4C9D-A5DD-149D77E233B1  2019-01-01 06:00:00.000   \n",
       "2  FCDD8B88-9BC5-4F84-BA71-610BFA889314  2019-01-01 06:00:00.000   \n",
       "3  9F461187-A082-42DF-B09B-7566B290657F  2019-01-01 06:00:00.000   \n",
       "4  3376B065-A0BD-4E3D-AC05-14B278077F90  2019-01-01 07:00:00.000   \n",
       "\n",
       "                                                Path  duration_ms  \\\n",
       "0  ../../../../data/samenvattingen/2019/01/01/E94...       331000   \n",
       "1  ../../../../data/samenvattingen/2019/01/01/168...        17000   \n",
       "2  ../../../../data/samenvattingen/2019/01/01/FCD...        17000   \n",
       "3  ../../../../data/samenvattingen/2019/01/01/9F4...       486000   \n",
       "4  ../../../../data/samenvattingen/2019/01/01/337...       333000   \n",
       "\n",
       "   adj_start_ms  adj_end_ms  \n",
       "0        310000      641000  \n",
       "1        137000      154000  \n",
       "2        163000      180000  \n",
       "3        310000      796000  \n",
       "4        310000      643000  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjust timings and create duration column\n",
    "if \"StartPosition\" in df.keys():\n",
    "    start_ms = df[\"StartPosition\"].apply(get_ms)\n",
    "    end_ms = df[\"EndPosition\"].apply(get_ms)\n",
    "    df[\"duration_ms\"] = end_ms - start_ms\n",
    "    df[\"adj_start_ms\"] = start_ms.apply(lambda x: adj_start_ms(x))\n",
    "    df[\"adj_end_ms\"] = df[\"adj_start_ms\"] + df[\"duration_ms\"]\n",
    "    df = df.drop(columns=[\"StartPosition\", \"EndPosition\", \"start_ms\", \"end_ms\"], errors=\"ignore\")\n",
    "# Drop rows with durations of 0\n",
    "df = df[df[\"duration_ms\"] > 0]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the transcripts from the json files\n",
    "- Only relevant transcripts for summaries\n",
    "- parse from \"text\" ipv \"result\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset JSON loader\n",
    "Recordings are divided into sentences like so:\n",
    "[\n",
    "    {\n",
    "        \"result\": [\n",
    "            [\n",
    "                \"word\",\n",
    "                starttime,\n",
    "                endtime,\n",
    "                confidence\n",
    "            ],\n",
    "            ...\n",
    "        ],\n",
    "        \"text\": entire sentence,\n",
    "        \"speaker\": \"unk\"??\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "\"\"\" \n",
    "def get_index(data, start, end) -> Tuple[int, int]:\n",
    "    # Get the beginning of sentence times\n",
    "    bos_times = []\n",
    "    for i in range(len(data)):\n",
    "        bos_times.append(data[i]['result'][0][1])\n",
    "    \n",
    "    # Find bos_time closest to start/end and return the index\n",
    "    if len(bos_times) > 0:\n",
    "        s = min(bos_times, key=lambda x:abs(x-start))\n",
    "        e = min(bos_times, key=lambda x:abs(x-end))\n",
    "        return bos_times.index(s), bos_times.index(e)\n",
    "    else:\n",
    "        return 0, 0\n",
    "\n",
    "def get_text_from_index(transcript, index):\n",
    "    \"\"\"\n",
    "    Get the relevant segment out of a full transcript\n",
    "    \"\"\"\n",
    "    start_index, end_index = index[0], index[1]\n",
    "    return transcript[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full transcript out of the .json file, also extract the relevant segment out of it.\n",
    "# Takes ~10 minutes.\n",
    "t_full = []\n",
    "t_sub = []\n",
    "t_indices = []\n",
    "for i, path in enumerate(df.Path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    index = get_index(data, df.adj_start_ms.iloc[i], df.adj_end_ms.iloc[i])\n",
    "    transcript = [t['text'] for t in data]\n",
    "    t_full.append(transcript)\n",
    "    t_sub.append(get_text_from_index(transcript, index))\n",
    "    t_indices.append(index)\n",
    "\n",
    "df['Transcript_full'] = t_full\n",
    "df['Transcript_sub'] = t_sub\n",
    "df['Transcript_indices'] = t_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ProgrammeDateTime</th>\n",
       "      <th>Path</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>adj_start_ms</th>\n",
       "      <th>adj_end_ms</th>\n",
       "      <th>Transcript_full</th>\n",
       "      <th>Transcript_sub</th>\n",
       "      <th>Transcript_indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E94B5735-A16D-41A9-A05A-09EE40E240A3</td>\n",
       "      <td>2019-01-01 05:00:00.000</td>\n",
       "      <td>../../../../data/samenvattingen/2019/01/01/E94...</td>\n",
       "      <td>331000</td>\n",
       "      <td>310000</td>\n",
       "      <td>641000</td>\n",
       "      <td>[Straat waren rode cirkels getekend en niemand...</td>\n",
       "      <td>[Nou ja goed, we gaan eens even kijken., Peter...</td>\n",
       "      <td>(34, 86)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9F461187-A082-42DF-B09B-7566B290657F</td>\n",
       "      <td>2019-01-01 06:00:00.000</td>\n",
       "      <td>../../../../data/samenvattingen/2019/01/01/9F4...</td>\n",
       "      <td>486000</td>\n",
       "      <td>310000</td>\n",
       "      <td>796000</td>\n",
       "      <td>[Een zinzen St Regis was dat is kwart over zes...</td>\n",
       "      <td>[André Meinema van de economieredactie die hee...</td>\n",
       "      <td>(41, 105)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3376B065-A0BD-4E3D-AC05-14B278077F90</td>\n",
       "      <td>2019-01-01 07:00:00.000</td>\n",
       "      <td>../../../../data/samenvattingen/2019/01/01/337...</td>\n",
       "      <td>333000</td>\n",
       "      <td>310000</td>\n",
       "      <td>643000</td>\n",
       "      <td>[En zeggen we waar we zijn en wat er aan de ha...</td>\n",
       "      <td>[Zeventien vuurwerk en oud en nieuw onlosmakel...</td>\n",
       "      <td>(72, 125)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>906D9A09-6E9B-4B01-ADEB-0A13D29D4A71</td>\n",
       "      <td>2019-01-01 07:00:00.000</td>\n",
       "      <td>../../../../data/samenvattingen/2019/01/01/906...</td>\n",
       "      <td>355000</td>\n",
       "      <td>259000</td>\n",
       "      <td>614000</td>\n",
       "      <td>[In tweeduizendnegentien reden we nog op benzi...</td>\n",
       "      <td>[Daar is de brandweer nog steeds bezig met het...</td>\n",
       "      <td>(48, 126)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BEDD587D-DC23-45BE-A3C5-8BFBB9FCBDE6</td>\n",
       "      <td>2019-01-01 08:00:00.000</td>\n",
       "      <td>../../../../data/samenvattingen/2019/01/01/BED...</td>\n",
       "      <td>33000</td>\n",
       "      <td>182000</td>\n",
       "      <td>215000</td>\n",
       "      <td>[Een reis op krans .nl ontdek Marokko met nu t...</td>\n",
       "      <td>[Het nieuwe jaar betekent ook dat er een aanta...</td>\n",
       "      <td>(37, 43)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     ID        ProgrammeDateTime  \\\n",
       "0  E94B5735-A16D-41A9-A05A-09EE40E240A3  2019-01-01 05:00:00.000   \n",
       "1  9F461187-A082-42DF-B09B-7566B290657F  2019-01-01 06:00:00.000   \n",
       "2  3376B065-A0BD-4E3D-AC05-14B278077F90  2019-01-01 07:00:00.000   \n",
       "3  906D9A09-6E9B-4B01-ADEB-0A13D29D4A71  2019-01-01 07:00:00.000   \n",
       "4  BEDD587D-DC23-45BE-A3C5-8BFBB9FCBDE6  2019-01-01 08:00:00.000   \n",
       "\n",
       "                                                Path  duration_ms  \\\n",
       "0  ../../../../data/samenvattingen/2019/01/01/E94...       331000   \n",
       "1  ../../../../data/samenvattingen/2019/01/01/9F4...       486000   \n",
       "2  ../../../../data/samenvattingen/2019/01/01/337...       333000   \n",
       "3  ../../../../data/samenvattingen/2019/01/01/906...       355000   \n",
       "4  ../../../../data/samenvattingen/2019/01/01/BED...        33000   \n",
       "\n",
       "   adj_start_ms  adj_end_ms  \\\n",
       "0        310000      641000   \n",
       "1        310000      796000   \n",
       "2        310000      643000   \n",
       "3        259000      614000   \n",
       "4        182000      215000   \n",
       "\n",
       "                                     Transcript_full  \\\n",
       "0  [Straat waren rode cirkels getekend en niemand...   \n",
       "1  [Een zinzen St Regis was dat is kwart over zes...   \n",
       "2  [En zeggen we waar we zijn en wat er aan de ha...   \n",
       "3  [In tweeduizendnegentien reden we nog op benzi...   \n",
       "4  [Een reis op krans .nl ontdek Marokko met nu t...   \n",
       "\n",
       "                                      Transcript_sub Transcript_indices  \n",
       "0  [Nou ja goed, we gaan eens even kijken., Peter...           (34, 86)  \n",
       "1  [André Meinema van de economieredactie die hee...          (41, 105)  \n",
       "2  [Zeventien vuurwerk en oud en nieuw onlosmakel...          (72, 125)  \n",
       "3  [Daar is de brandweer nog steeds bezig met het...          (48, 126)  \n",
       "4  [Het nieuwe jaar betekent ook dat er een aanta...           (37, 43)  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final dataframe cleaned from (near) empty transcripts\n",
    "empty = df[df.Transcript_sub.apply(lambda x: len(x) < 4)]\n",
    "df.drop(empty.index, axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.head()\n",
    "\n",
    "# Check for empty transcripts\n",
    "# uncomment to save, already saved in csv!\n",
    "# dfc.to_csv(\"/AI/data/samenvattingen/csv/data_2019_04_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of segments: 98530\n",
      "Avg. segment length: 48.92\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of segments: {len(df)}')\n",
    "print(f'Avg. segment length: {np.mean(df.Transcript_sub.apply(lambda x: len(x))):.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_SECTIONS = 2\n",
    "MAX_SECTIONS = 5\n",
    "\n",
    "def create_docs(data: pd.DataFrame, save=True) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Creates a folder containing .txt files from concatenated segments.\n",
    "    :param data: Dataframe containing the data to be processed.\n",
    "    :param save: Whether to save to a folder or to return the results as a list of Strings.\n",
    "    \"\"\"\n",
    "    d = data.copy()\n",
    "    i = 0\n",
    "    pad = int(np.log10(len(d))) + 1\n",
    "\n",
    "    if save:\n",
    "        os.makedirs(NLAUVI_LOC_C, exist_ok=True)\n",
    "    else:\n",
    "        docs = []\n",
    "\n",
    "    with tqdm(total=len(d), desc='Transcripts processed') as pbar:\n",
    "        while len(d) > 0:\n",
    "            if len(d) > MAX_SECTIONS + 1: # +1 to prevent being left with a single segment\n",
    "                n = rng.integers(MIN_SECTIONS, MAX_SECTIONS + 1)\n",
    "            elif len(d) > MAX_SECTIONS:\n",
    "                n = rng.integers(MIN_SECTIONS, MAX_SECTIONS)\n",
    "            else:\n",
    "                n = len(d)\n",
    "            slice = rng.choice(len(d), size=n, replace=False)\n",
    "\n",
    "            doc = ''\n",
    "            l = 0\n",
    "            for t in slice:\n",
    "                text = d[\"Transcript_sub\"].iloc[t]\n",
    "                doc += f'==={d[\"ID\"].iloc[t]}===\\n{\" \".join(text)}\\n'\n",
    "                l += len(text)\n",
    "\n",
    "            d.drop(slice, inplace=True)\n",
    "            d.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            if save:\n",
    "                with open(f'{NLAUVI_LOC_C}/{i:0{pad}}', 'w') as tfile:\n",
    "                    tfile.write(doc)\n",
    "            else:\n",
    "                docs.append(doc)\n",
    "            i += 1\n",
    "            pbar.update(n)\n",
    "\n",
    "    print(f'Created {i} Documents')\n",
    "\n",
    "    if save is False: \n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcripts processed: 100%|██████████| 98530/98530 [04:30<00:00, 363.76it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 28217 Documents\n",
      "   Average document length = 48.88778218804267 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_docs(df)\n",
    "subdivide_dir(NLAUVI_LOC_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Dataset\n",
    "Unconcatenated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs_normal(data: pd.DataFrame, n: Optional[int] = None, save=True) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Creates a folder containing .txt files from unaltered segments.\n",
    "    :param data: Dataframe containing the data to be processed.\n",
    "    :param save: Whether to save to a folder or to return the results as a list of Strings.\n",
    "    \"\"\"\n",
    "    d = data.copy()\n",
    "    if n:\n",
    "        d = d[:n]\n",
    "    pad = int(np.log10(len(d))) + 1\n",
    "\n",
    "    if save:\n",
    "        os.makedirs('../'+NLAUVI_LOC_N, exist_ok=True)\n",
    "    else:\n",
    "        docs = []\n",
    "\n",
    "    pbar = tqdm(d.itertuples(), total=len(d), desc='Transcripts processed')\n",
    "    for i, row in enumerate(pbar):\n",
    "        doc = ''\n",
    "        sections = []\n",
    "        sections.append(' '.join(row.Transcript_full[0:row.Transcript_indices[0]]))\n",
    "        sections.append(' '.join(row.Transcript_full[row.Transcript_indices[0]:row.Transcript_indices[1]]))\n",
    "        sections.append(' '.join(row.Transcript_full[row.Transcript_indices[1]:]))\n",
    "        if len(sections) < 2:\n",
    "            raise ValueError('Less than 2 sections')\n",
    "        for s in sections:\n",
    "            doc += f'==={row.ID}===\\n{s}\\n'\n",
    "        \n",
    "        if save:\n",
    "            with open(f'../{NLAUVI_LOC_N}/{i:0{pad}}', 'w') as tfile:\n",
    "                tfile.write(doc)\n",
    "        else:\n",
    "            docs.append(doc)\n",
    "\n",
    "    print(f'Created {i} Documents')\n",
    "\n",
    "    if save is False: \n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcripts processed: 100%|██████████| 98530/98530 [00:07<00:00, 13293.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 98529 Documents\n"
     ]
    }
   ],
   "source": [
    "create_docs_normal(df)\n",
    "subdivide_dir('../'+NLAUVI_LOC_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ts_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bafe9ace8fecc43223ecf8a2ac5adce83c42b210aab5a7d7f6b6dc668b624016"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
